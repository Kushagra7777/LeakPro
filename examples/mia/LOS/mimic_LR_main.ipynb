{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# MIA attacks on Length-of-Stay predictor, Logistic Regression\n",
            "## Installation of Packages in Conda\n",
            "\n",
            "To install the required packages in your conda environment, you can use the following commands:\n",
            "\n",
            "```bash\n",
            "conda install h5py\n",
            "conda install pytables\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import sys\n",
            "\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
            "if project_root not in sys.path:\n",
            "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train the classifier\n",
            "For the LR, the data should be flatten. So set the value to True for the LR model anb False for the GRU-D"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Load the dataset\n",
            "The dataset is generated by the notebook file ...."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loading dataset...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
                  "  warn(\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loaded dataset and indices from ./data/LR_data\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "import yaml\n",
            "import pickle\n",
            "\n",
            "# Load the config.yaml file\n",
            "with open(\"train_config.yaml\", \"r\") as file:\n",
            "    train_config = yaml.safe_load(file)\n",
            "\n",
            "# Determine training method and paths\n",
            "use_LR = train_config['train']['training_method'] == 'LR'\n",
            "data_path = train_config['data']['data_dir']\n",
            "path = os.path.join(data_path, \"LR_data\" if use_LR else \"GRUD_data\")\n",
            "\n",
            "# File paths\n",
            "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
            "indices_path = os.path.join(path, \"indices.pkl\")\n",
            "\n",
            "# Load dataset and indices\n",
            "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
            "    print(\"Loading dataset...\")\n",
            "    \n",
            "    with open(dataset_path, \"rb\") as f:\n",
            "        dataset = pickle.load(f)\n",
            "\n",
            "    with open(indices_path, \"rb\") as f:\n",
            "        indices_dict = pickle.load(f)\n",
            "        train_indices = indices_dict[\"train_indices\"]\n",
            "        test_indices = indices_dict[\"test_indices\"]\n",
            "        early_stop_indices = indices_dict[\"early_stop_indices\"]\n",
            "        #TODO: fix this\n",
            "        data_indices = train_indices + test_indices + early_stop_indices\n",
            "\n",
            "    print(f\"Loaded dataset and indices from {path}\")\n",
            "else:\n",
            "    print(\"Dataset not found.\\nâ†’ Run 'mimic_dataset_prep.ipynb' to generate the required dataset.\\n\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Create dala loaders."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import DataLoader\n",
            "from mimic_handler import MIMICInputHandler\n",
            "\n",
            "\n",
            "data = dataset.data\n",
            "targets = dataset.targets\n",
            "\n",
            "train_subset = MIMICInputHandler.UserDataset(data[train_indices], targets[train_indices])\n",
            "test_subset = MIMICInputHandler.UserDataset(data[test_indices], targets[test_indices])\n",
            "early_stop_subset = MIMICInputHandler.UserDataset(data[early_stop_indices], targets[early_stop_indices])\n",
            "\n",
            "# Create DataLoaders\n",
            "batch_size = train_config['data']['batch_size']\n",
            "train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
            "test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
            "early_stop_loader = DataLoader(early_stop_subset, batch_size=batch_size)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "lr  = 0,0001 for LR - weight_decay = 5.392, epochs = 20"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Number of features: 7488\n"
               ]
            },
            {
               "ename": "AttributeError",
               "evalue": "'MIMICInputHandler' object has no attribute 'train_GRU'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mMIMICInputHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m test_results \u001b[38;5;241m=\u001b[39m MIMICInputHandler()\u001b[38;5;241m.\u001b[39meval(test_loader, model, criterion)\n",
                  "File \u001b[0;32m~/LeakPro/examples/mia/LOS/mimic_handler.py:19\u001b[0m, in \u001b[0;36mMIMICInputHandler.train\u001b[0;34m(self, dataloader, model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader, model, criterion, optimizer, epochs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TrainingOutput:\n\u001b[1;32m     15\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     16\u001b[0m     train_fn_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_LR,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRUD\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_GRUD,\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_GRU\u001b[49m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_fn_map:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                  "\u001b[0;31mAttributeError\u001b[0m: 'MIMICInputHandler' object has no attribute 'train_GRU'"
               ]
            }
         ],
         "source": [
            "if use_LR:\n",
            "    from target_model_class import LR\n",
            "    from torch import  nn, optim, save\n",
            "\n",
            "\n",
            "    # Create model\n",
            "    n_features = train_subset.data.shape[1]\n",
            "    print(f\"Number of features: {n_features}\")\n",
            "    model = LR(input_dim = n_features)\n",
            "\n",
            "    # Read parameters from config file\n",
            "    lr = train_config['train']['LR']['learning_rate']\n",
            "    weight_decay = train_config['train']['LR']['weight_decay']\n",
            "    epochs = train_config['train']['LR']['epochs']\n",
            "\n",
            "    # Create optimizer\n",
            "    criterion = nn.BCELoss()\n",
            "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
            "\n",
            "    # Train the model\n",
            "    train_results = MIMICInputHandler().train(train_loader, model, criterion, optimizer, epochs)\n",
            "    \n",
            "    # Evaluate the model\n",
            "    test_results = MIMICInputHandler().eval(test_loader, model, criterion)\n",
            "\n",
            "    # Store model and its metadata\n",
            "    model = train_results.model\n",
            "    model.to(\"cpu\")\n",
            "    target_dir = \"target_LR\"\n",
            "    os.makedirs(target_dir, exist_ok=True)\n",
            "    with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
            "        save(model.state_dict(), f)\n",
            "\n",
            "    # Create metadata to be used by LeakPro\n",
            "    from leakpro import LeakPro\n",
            "    meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
            "                                        optimizer = optimizer,\n",
            "                                        loss_fn = criterion,\n",
            "                                        dataloader = train_loader,\n",
            "                                        test_result = test_results,\n",
            "                                        epochs = epochs,\n",
            "                                        train_indices = train_indices,\n",
            "                                        test_indices = test_indices,\n",
            "                                        dataset_name = train_config[\"data\"][\"dataset\"])\n",
            "\n",
            "    with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
            "        pickle.dump(meta_data, f)\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "train_acc = train_results.metrics.extra[\"accuracy_history\"]\n",
            "train_loss = train_results.metrics.extra[\"loss_history\"]\n",
            "test_acc = test_results.accuracy\n",
            "test_loss = test_results.loss\n",
            "\n",
            "# Plot training and test accuracy\n",
            "plt.figure(figsize=(5, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.plot(train_acc, label='Train Accuracy')\n",
            "plt.plot(len(train_loss)-1, test_acc, 'ro', label='Test Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.title('Accuracy over Epochs')\n",
            "plt.legend()\n",
            "\n",
            "# Plot training and test loss\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.plot(train_loss, label='Train Loss')\n",
            "plt.plot(len(train_loss)-1, test_loss, 'ro', label='Test Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.title('Loss over Epochs')\n",
            "plt.legend()\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Attack the LR model\n",
            "Modify ```audit.yaml ``` file to attack LR model: \n",
            "  \n",
            "  ```\n",
            "  module_path: \"utils/model_LR.py\" \n",
            "  model_class: \"LR\"\n",
            "  target_folder: \"./target_LR\"\n",
            "  data_path: \"./data/LR_data/dataset.pkl\"\n",
            "  ```\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import sys\n",
            "\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
            "if project_root not in sys.path:\n",
            "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n",
            "# Read the config file\n",
            "config_path = \"audit.yaml\"\n",
            "from leakpro import LeakPro\n",
            "from mimic_handler import MIMICInputHandler\n",
            "\n",
            "\n",
            "# Instantiate leakpro object\n",
            "leakpro = LeakPro(MIMICInputHandler, config_path)\n",
            "\n",
            "# Run the audit \n",
            "mia_results = leakpro.run_audit(return_results=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from mimic_LR_handler import MimicInputHandler\n",
            "\n",
            "from leakpro import LeakPro\n",
            "\n",
            "# Read the config file\n",
            "config_path = \"audit.yaml\"\n",
            "\n",
            "# Prepare leakpro object\n",
            "leakpro = LeakPro(MimicInputHandler, config_path)\n",
            "\n",
            "# Run the audit\n",
            "mia_results = leakpro.run_audit(return_results=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate report"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Import and initialize ReportHandler\n",
            "from leakpro.reporting.report_handler import ReportHandler\n",
            "\n",
            "# report_handler = ReportHandler()\n",
            "report_handler = ReportHandler(report_dir=\"./leakpro_output/results\")\n",
            "\n",
            "# Save MIA resuls using report handler\n",
            "for res in mia_results:\n",
            "    report_handler.save_results(attack_name=res.attack_name, result_data=res, config=res.configs)\n",
            "\n",
            "# # Create the report by compiling the latex text\n",
            "report_handler.create_report()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".leakpro_dev",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
