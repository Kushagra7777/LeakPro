{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIA attacks on Length-of-Stay predictor, Gated Recurrent Unit with Decay (GRU-D)\n",
    "## Installation of Packages in Conda\n",
    "\n",
    "To install the required packages in your conda environment, you can use the following commands:\n",
    "\n",
    "```bash\n",
    "conda install h5py\n",
    "conda install pytables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier\n",
    "### Load the dataset\n",
    "The dataset is generated by the notebook file `mimic_dataset_prep.ipynb`.\\\n",
    "In `train_config.yaml` set the `training_method` to `GRUD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open(\"train_config.yaml\", \"r\") as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "\n",
    "# Determine training method and paths\n",
    "#TODO: DO we want GRUD traning mode in the config file or not?\n",
    "assert train_config['train']['training_method'] == 'GRUD', \"The training config is not set to GRUD\"\n",
    "use_LR = False\n",
    "data_path = train_config['data']['data_dir']\n",
    "path = os.path.join(data_path, \"LR_data\" if use_LR else \"GRUD_data\")\n",
    "\n",
    "# File paths\n",
    "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
    "indices_path = os.path.join(path, \"indices.pkl\")\n",
    "\n",
    "# Load dataset and indices\n",
    "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    with open(indices_path, \"rb\") as f:\n",
    "        indices_dict = pickle.load(f)\n",
    "        train_indices = indices_dict[\"train_indices\"]\n",
    "        test_indices = indices_dict[\"test_indices\"]\n",
    "        early_stop_indices = indices_dict[\"early_stop_indices\"]\n",
    "        #TODO: fix this\n",
    "        data_indices = train_indices + test_indices + early_stop_indices\n",
    "\n",
    "    print(f\"Loaded dataset and indices from {path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset not found.\\nâ†’ Run 'mimic_dataset_prep.ipynb' to generate the required dataset.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dala loaders. The  `batch_size` is one of the parameters which is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from mimic_data_handler import MIMICUserDataset\n",
    "\n",
    "\n",
    "data = dataset.data\n",
    "targets = dataset.targets\n",
    "\n",
    "train_subset = MIMICUserDataset(data[train_indices], targets[train_indices])\n",
    "test_subset = MIMICUserDataset(data[test_indices], targets[test_indices])\n",
    "early_stop_subset = MIMICUserDataset(data[early_stop_indices], targets[early_stop_indices])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 59\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
    "early_stop_loader = DataLoader(early_stop_subset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimized_hyperparams` is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"hidden_size\": 78,\n",
    "    \"learning_rate\": 0.00473,\n",
    "    \"num_epochs\": 150,\n",
    "    \"patience_early_stopping\": 40,\n",
    "    \"patience_lr_scheduler\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": 4410,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import  nn, optim, save, zeros\n",
    "from mimic_model_handler import GRUHandler\n",
    "from target_models import GRUD\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params = {\n",
    "    \"hidden_size\": optimized_hyperparams[\"hidden_size\"],\n",
    "    \"batch_size\": optimized_hyperparams[\"batch_size\"],\n",
    "    \"input_size\": int(data.shape[1]/3),\n",
    "    \"X_mean\":  zeros(1,data.shape[2],int(data.shape[1]/3)),\n",
    "    \"output_last\": False,\n",
    "    \"bn_flag\": True,\n",
    "}\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimized_hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Train the model\n",
    "train_results = GRUHandler().train(train_loader,\n",
    "                                    model,\n",
    "                                    criterion,\n",
    "                                    optimizer,\n",
    "                                    optimized_hyperparams[\"num_epochs\"],\n",
    "                                    early_stop_loader,\n",
    "                                    optimized_hyperparams[\"patience_early_stopping\"],\n",
    "                                    optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                    optimized_hyperparams[\"min_delta\"],\n",
    "                                    )\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = GRUHandler().eval(test_loader, model, criterion)\n",
    "\n",
    "\n",
    "# Store model and its metadata\n",
    "model = train_results.model\n",
    "model.to(\"cpu\")\n",
    "target_dir = \"target_GRUD\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
    "    save(model.state_dict(), f)\n",
    "\n",
    "# Create metadata to be used by LeakPro\n",
    "from leakpro import LeakPro\n",
    "meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
    "                                    optimizer = optimizer,\n",
    "                                    loss_fn = criterion,\n",
    "                                    dataloader = train_loader,\n",
    "                                    test_result = test_results,\n",
    "                                    epochs = optimized_hyperparams[\"num_epochs\"],\n",
    "                                    train_indices = train_indices,\n",
    "                                    test_indices = test_indices,\n",
    "                                    dataset_name = train_config[\"data\"][\"dataset\"])\n",
    "\n",
    "with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = train_results.metrics.extra[\"accuracy_history\"]\n",
    "train_loss = train_results.metrics.extra[\"loss_history\"]\n",
    "test_acc = test_results.accuracy\n",
    "test_loss = test_results.loss\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(len(train_loss)-1, test_acc, 'ro', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(len(train_loss)-1, test_loss, 'ro', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attacking the GRUD model\n",
    "Modify ```audit.yaml ``` file to attack GRUD model: \n",
    "  \n",
    "  ```\n",
    "  model_class: \"GRUD\"\n",
    "  target_folder: \"./target_GRUD\"\n",
    "  data_path: \"./data/GRUD_data/dataset.pkl\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 12:26:45,486 INFO     Target model blueprint created from GRUD in ./target_models.py.\n",
      "06/10/2025 12:26:45:INFO:Target model blueprint created from GRUD in ./target_models.py.\n",
      "2025-06-10 12:26:45,497 INFO     Loaded target model metadata from ./target_GRUD/model_metadata.pkl\n",
      "06/10/2025 12:26:45:INFO:Loaded target model metadata from ./target_GRUD/model_metadata.pkl\n",
      "/home/fazeleh/LeakPro/leakpro/input_handler/mia_handler.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.target_model.load_state_dict(torch.load(f))\n",
      "2025-06-10 12:26:45,500 INFO     Loaded target model from ./target_GRUD\n",
      "06/10/2025 12:26:45:INFO:Loaded target model from ./target_GRUD\n",
      "2025-06-10 12:26:46,245 INFO     Loaded population dataset from ./data/GRUD_data/dataset.pkl\n",
      "06/10/2025 12:26:46:INFO:Loaded population dataset from ./data/GRUD_data/dataset.pkl\n",
      "2025-06-10 12:26:46,247 INFO     Image extension initialized.\n",
      "06/10/2025 12:26:46:INFO:Image extension initialized.\n",
      "2025-06-10 12:26:46,262 WARNING  No one-hot encoding information found in the population object.\n",
      "06/10/2025 12:26:46:WARNING:No one-hot encoding information found in the population object.\n",
      "2025-06-10 12:26:46,263 INFO     MIA attack factory loaded.\n",
      "06/10/2025 12:26:46:INFO:MIA attack factory loaded.\n",
      "2025-06-10 12:26:46,264 INFO     Shadow model handler singleton already exists, updating state\n",
      "06/10/2025 12:26:46:INFO:Shadow model handler singleton already exists, updating state\n",
      "2025-06-10 12:26:46,266 INFO     Distillation model handler singleton already exists, updating state\n",
      "06/10/2025 12:26:46:INFO:Distillation model handler singleton already exists, updating state\n",
      "2025-06-10 12:26:46,268 INFO     Configuring the RMIA attack\n",
      "06/10/2025 12:26:46:INFO:Configuring the RMIA attack\n",
      "2025-06-10 12:26:46,269 INFO     Added attack: rmia\n",
      "06/10/2025 12:26:46:INFO:Added attack: rmia\n",
      "2025-06-10 12:26:46,269 INFO     Shadow model handler singleton already exists, updating state\n",
      "06/10/2025 12:26:46:INFO:Shadow model handler singleton already exists, updating state\n",
      "2025-06-10 12:26:46,271 INFO     Distillation model handler singleton already exists, updating state\n",
      "06/10/2025 12:26:46:INFO:Distillation model handler singleton already exists, updating state\n",
      "2025-06-10 12:26:46,273 INFO     Added attack: lira\n",
      "06/10/2025 12:26:46:INFO:Added attack: lira\n",
      "2025-06-10 12:26:46,286 INFO     Loaded previous results for attack: rmia\n",
      "06/10/2025 12:26:46:INFO:Loaded previous results for attack: rmia\n",
      "2025-06-10 12:26:46,321 INFO     Loaded previous results for attack: lira\n",
      "06/10/2025 12:26:46:INFO:Loaded previous results for attack: lira\n",
      "2025-06-10 12:26:46,323 INFO     Creating PDF report\n",
      "06/10/2025 12:26:46:INFO:Creating PDF report\n",
      "2025-06-10 12:26:46,323 INFO     Initializing report handler...\n",
      "06/10/2025 12:26:46:INFO:Initializing report handler...\n",
      "2025-06-10 12:26:46,324 INFO     report_dir set to: ./leakpro_output/results\n",
      "06/10/2025 12:26:46:INFO:report_dir set to: ./leakpro_output/results\n",
      "2025-06-10 12:26:52,430 INFO     No results of type GIAResults found.\n",
      "06/10/2025 12:26:52:INFO:No results of type GIAResults found.\n",
      "2025-06-10 12:26:52,431 INFO     No results of type SinglingOutResults found.\n",
      "06/10/2025 12:26:52:INFO:No results of type SinglingOutResults found.\n",
      "2025-06-10 12:26:52,432 INFO     No results of type InferenceResults found.\n",
      "06/10/2025 12:26:52:INFO:No results of type InferenceResults found.\n",
      "2025-06-10 12:26:52,432 INFO     No results of type LinkabilityResults found.\n",
      "06/10/2025 12:26:52:INFO:No results of type LinkabilityResults found.\n",
      "2025-06-10 12:26:57,668 INFO     PDF compiled\n",
      "06/10/2025 12:26:57:INFO:PDF compiled\n",
      "2025-06-10 12:26:57,669 INFO     Auditing completed\n",
      "06/10/2025 12:26:57:INFO:Auditing completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from leakpro import LeakPro\n",
    "from mimic_model_handler import GRUHandler as InputHandler\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Instantiate leakpro object\n",
    "leakpro = LeakPro(InputHandler, config_path)\n",
    "\n",
    "# Run the audit \n",
    "mia_results = leakpro.run_audit(create_pdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".leakpro_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
