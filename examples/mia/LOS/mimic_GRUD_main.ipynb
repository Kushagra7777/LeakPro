{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIA attacks on Length-of-Stay predictor, Gated Recurrent Unit with Decay (GRU-D)\n",
    "## Installation of Packages in Conda\n",
    "\n",
    "To install the required packages in your conda environment, you can use the following commands:\n",
    "\n",
    "```bash\n",
    "conda install h5py\n",
    "conda install pytables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier\n",
    "### Load the dataset\n",
    "The dataset is generated by the notebook file `mimic_dataset_prep.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset and indices from ./data/GRUD_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open(\"train_config.yaml\", \"r\") as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "\n",
    "# Determine training method and paths\n",
    "#TODO: DO we want GRUD traning mode in the config file or not?\n",
    "use_LR = train_config['train']['training_method'] == 'LR'\n",
    "data_path = train_config['data']['data_dir']\n",
    "path = os.path.join(data_path, \"LR_data\" if use_LR else \"GRUD_data\")\n",
    "\n",
    "# File paths\n",
    "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
    "indices_path = os.path.join(path, \"indices.pkl\")\n",
    "\n",
    "# Load dataset and indices\n",
    "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    with open(indices_path, \"rb\") as f:\n",
    "        indices_dict = pickle.load(f)\n",
    "        train_indices = indices_dict[\"train_indices\"]\n",
    "        test_indices = indices_dict[\"test_indices\"]\n",
    "        early_stop_indices = indices_dict[\"early_stop_indices\"]\n",
    "        #TODO: fix this\n",
    "        data_indices = train_indices + test_indices + early_stop_indices\n",
    "\n",
    "    print(f\"Loaded dataset and indices from {path}\")\n",
    "else:\n",
    "    print(\"Dataset not found.\\nâ†’ Run 'mimic_dataset_prep.ipynb' to generate the required dataset.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dala loaders. The  `batch_size` is one of the parameters which is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from mimic_data_handler import MIMICUserDataset\n",
    "\n",
    "\n",
    "data = dataset.data\n",
    "targets = dataset.targets\n",
    "\n",
    "train_subset = MIMICUserDataset(data[train_indices], targets[train_indices])\n",
    "test_subset = MIMICUserDataset(data[test_indices], targets[test_indices])\n",
    "early_stop_subset = MIMICUserDataset(data[early_stop_indices], targets[early_stop_indices])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 74\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
    "early_stop_loader = DataLoader(early_stop_subset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimized_hyperparams` is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"hidden_size\": 27,\n",
    "    \"learning_rate\": 0.000289,\n",
    "    \"num_epochs\":40,\n",
    "    \"patience_early_stopping\": 40,\n",
    "    \"patience_lr_scheduler\": 2,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": 6286,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GRUHandler.train() takes from 6 to 9 positional arguments but 11 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39moptimized_hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience_lr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mGRUHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mearly_stop_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimized_hyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimized_hyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience_early_stopping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimized_hyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience_lr_scheduler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimized_hyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimized_hyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m test_results \u001b[38;5;241m=\u001b[39m GRUHandler()\u001b[38;5;241m.\u001b[39meval(test_loader, model, criterion)\n",
      "\u001b[0;31mTypeError\u001b[0m: GRUHandler.train() takes from 6 to 9 positional arguments but 11 were given"
     ]
    }
   ],
   "source": [
    "from target_model_class import LR\n",
    "from torch import  nn, optim, save, zeros\n",
    "from mimic_model_handler import GRUHandler\n",
    "from utils.model_GRUD import *\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params = {\n",
    "    \"hidden_size\": optimized_hyperparams[\"hidden_size\"],\n",
    "    \"batch_size\": optimized_hyperparams[\"batch_size\"],\n",
    "    \"input_size\": int(data.shape[1]/3),\n",
    "    \"X_mean\":  zeros(1,data.shape[2],int(data.shape[1]/3)),\n",
    "    \"output_last\": False,\n",
    "    \"bn_flag\": True,\n",
    "}\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimized_hyperparams[\"patience_lr_scheduler\"])\n",
    "\n",
    "# Train the model\n",
    "train_results = GRUHandler().train(model,\n",
    "                                    train_loader,\n",
    "                                    criterion,\n",
    "                                    optimizer,\n",
    "                                    early_stop_loader,\n",
    "                                    optimized_hyperparams[\"num_epochs\"],\n",
    "                                    optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                    optimized_hyperparams[\"min_delta\"],\n",
    "                                    )\n",
    "    def train(self,\n",
    "              model: nn.Module,\n",
    "              dataloader: DataLoader,\n",
    "              criterion: nn.Module ,\n",
    "              optimizer: optim.Optimizer ,\n",
    "              early_stop_loader:DataLoader,\n",
    "              epochs: int = None,\n",
    "              patience_lr: float = 0.01,\n",
    "              min_delta: float = 0.0,\n",
    "              ) -> TrainingOutput:\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = GRUHandler().eval(test_loader, model, criterion)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store model and its metadata\n",
    "model = train_results.model\n",
    "model.to(\"cpu\")\n",
    "target_dir = \"target_GRUD\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
    "    save(model.state_dict(), f)\n",
    "\n",
    "# Create metadata to be used by LeakPro\n",
    "from leakpro import LeakPro\n",
    "meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
    "                                    optimizer = optimizer,\n",
    "                                    loss_fn = criterion,\n",
    "                                    dataloader = train_loader,\n",
    "                                    test_result = test_results,\n",
    "                                    epochs = epochs,\n",
    "                                    train_indices = train_indices,\n",
    "                                    test_indices = test_indices,\n",
    "                                    dataset_name = train_config[\"data\"][\"dataset\"])\n",
    "\n",
    "with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_model_class import LR\n",
    "from torch import  nn, optim, save, zeros\n",
    "from examples.mia.LOS.mimic_LR_handler_del import MIMICLRHandler\n",
    "\n",
    "\n",
    "# Create model\n",
    "n_features = train_subset.data.shape[1]\n",
    "print(f\"Number of features: {n_features}\")\n",
    "model = LR(input_dim = n_features)\n",
    "\n",
    "# Read parameters from config file\n",
    "lr = train_config['train']['LR']['learning_rate']\n",
    "weight_decay = train_config['train']['LR']['weight_decay']\n",
    "epochs = train_config['train']['LR']['epochs']\n",
    "\n",
    "# Create optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Train the model\n",
    "train_results = MIMICLRHandler().train(train_loader, model, criterion, optimizer, epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = MIMICLRHandler().eval(test_loader, model, criterion)\n",
    "\n",
    "# Store model and its metadata\n",
    "model = train_results.model\n",
    "model.to(\"cpu\")\n",
    "target_dir = \"target_LR\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
    "    save(model.state_dict(), f)\n",
    "\n",
    "# Create metadata to be used by LeakPro\n",
    "from leakpro import LeakPro\n",
    "meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
    "                                    optimizer = optimizer,\n",
    "                                    loss_fn = criterion,\n",
    "                                    dataloader = train_loader,\n",
    "                                    test_result = test_results,\n",
    "                                    epochs = epochs,\n",
    "                                    train_indices = train_indices,\n",
    "                                    test_indices = test_indices,\n",
    "                                    dataset_name = train_config[\"data\"][\"dataset\"])\n",
    "\n",
    "with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "train_frac = 0.4\n",
    "valid_frac = 0.0\n",
    "test_frac = 0.0\n",
    "early_stop_frac = 0.4\n",
    "batch_size = 74\n",
    "use_LR = False # True if you want to use the LR model, False if you want to use the GRUD model\n",
    "\n",
    "dataset, train_indices, validation_indices, test_indices, early_stop_indices= get_mimic_dataset(path,\n",
    "                                                                            train_frac ,\n",
    "                                                                            valid_frac,\n",
    "                                                                            test_frac,\n",
    "                                                                            early_stop_frac,\n",
    "                                                                            use_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
    "                                                            train_indices,\n",
    "                                                            validation_indices,\n",
    "                                                            test_indices,\n",
    "                                                            early_stop_indices,\n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimized_hyperparams` is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"hidden_size\": 27,\n",
    "    \"learning_rate\": 0.000289,\n",
    "    \"num_epochs\":40,\n",
    "    \"patience_early_stopping\": 40,\n",
    "    \"patience_lr_scheduler\": 2,\n",
    "    \"batch_size\": 74,\n",
    "    \"seed\": 6286,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }\n",
    "\n",
    "n_features = int(dataset.x.shape[1]/3)\n",
    "X_mean = zeros(1,dataset.x.shape[2],n_features)\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params = {\n",
    "    \"hidden_size\": optimized_hyperparams[\"hidden_size\"],\n",
    "    \"batch_size\": optimized_hyperparams[\"batch_size\"],\n",
    "    \"input_size\": n_features,\n",
    "    \"X_mean\": X_mean,\n",
    "    \"output_last\": False,\n",
    "    \"bn_flag\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_GRUD import *\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "\n",
    "# Train the model with Train_Model function\n",
    "train_losses, test_losses , train_acc, test_acc = gru_trained_model_and_metadata(model,\n",
    "                                                                                train_loader,\n",
    "                                                                                early_stop_loader,\n",
    "                                                                                epochs = optimized_hyperparams[\"num_epochs\"],\n",
    "                                                                                patience_early_stopping = optimized_hyperparams[\"patience_early_stopping\"],\n",
    "                                                                                patience_lr= optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                                                                min_delta = optimized_hyperparams[\"min_delta\"],\n",
    "                                                                                learning_rate = optimized_hyperparams[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert losses to numpy-compatible lists directly\n",
    "train_losses_cpu = [float(loss) for loss in train_losses]\n",
    "test_losses_cpu = [float(loss) for loss in test_losses]\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attacking the GRUD model\n",
    "Modify ```audit.yaml ``` file to attack GRUD model: \n",
    "  \n",
    "  ```\n",
    "  module_path: \"utils/model_GRUD.py\" \n",
    "  model_class: \"GRUD\"\n",
    "  target_folder: \"./target_GRUD\"\n",
    "  data_path: \"./data/GRUD_data/dataset.pkl\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic_GRUD_handler import MimicInputHandlerGRU\n",
    "\n",
    "from leakpro import LeakPro\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Prepare leakpro object\n",
    "leakpro = LeakPro(MimicInputHandlerGRU, config_path)\n",
    "\n",
    "# Run the audit\n",
    "mia_results = leakpro.run_audit(return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import and initialize ReportHandler\n",
    "from leakpro.reporting.report_handler import ReportHandler\n",
    "\n",
    "# report_handler = ReportHandler()\n",
    "report_handler = ReportHandler(report_dir=\"./leakpro_output/results\")\n",
    "\n",
    "# Save MIA resuls using report handler\n",
    "for res in mia_results:\n",
    "    report_handler.save_results(attack_name=res.attack_name, result_data=res, config=res.configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the report by compiling the latex text\n",
    "report_handler.create_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".leakpro_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
