{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LOT perdiciton Model Training\n",
    "The task is to predict if the LOS > 3 Days by logistic regression.\n",
    "\n",
    "This document is a based on [this code](https://github.com/MLforHealth/MIMIC_Extract/blob/master/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20Sklearn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, pandas as pd, numpy as np, scipy.stats as ss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5993718fc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILEPATH     = './mimic_data/final/grouping_5/all_hourly_data.h5'\n",
    "RAW_DATA_FILEPATH = './mimic_data/final/nogrouping_5/all_hourly_data.h5'\n",
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '2'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_lvl2 = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "data_full_raw  = pd.read_hdf(RAW_DATA_FILEPATH, 'vitals_labs') \n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_lvl2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fazeleh/miniconda3/envs/mimic_test/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2, raw = [df[\n",
    "    (df.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (df.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "] for df in (data_full_lvl2, data_full_raw)]\n",
    "\n",
    "raw.columns = raw.columns.droplevel(level=['label', 'LEVEL1', 'LEVEL2'])\n",
    "\n",
    "train_frac, dev_frac, test_frac = 0.7, 0.1, 0.2\n",
    "lvl2_subj_idx, raw_subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, raw, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "assert lvl2_subjects == set(raw_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "N_train, N_dev, N_test = int(train_frac * N), int(dev_frac * N), int(test_frac * N)\n",
    "train_subj = subjects[:N_train]\n",
    "dev_subj   = subjects[N_train:N_train + N_dev]\n",
    "test_subj  = subjects[N_train+N_dev:]\n",
    "\n",
    "[(lvl2_train, lvl2_dev, lvl2_test), (raw_train, raw_dev, raw_test), (Ys_train, Ys_dev, Ys_test)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj, dev_subj, test_subj)] \\\n",
    "    for df in (lvl2, raw, Ys)\n",
    "]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2_train.loc[:, idx[:,'mean']].mean(axis=0), lvl2_train.loc[:, idx[:,'mean']].std(axis=0)\n",
    "raw_means, raw_stds = raw_train.loc[:, idx[:,'mean']].mean(axis=0), raw_train.loc[:, idx[:,'mean']].std(axis=0)\n",
    "\n",
    "lvl2_train.loc[:, idx[:,'mean']] = (lvl2_train.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "lvl2_dev.loc[:, idx[:,'mean']] = (lvl2_dev.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "lvl2_test.loc[:, idx[:,'mean']] = (lvl2_test.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "\n",
    "raw_train.loc[:, idx[:,'mean']] = (raw_train.loc[:, idx[:,'mean']] - raw_means)/raw_stds\n",
    "raw_dev.loc[:, idx[:,'mean']] = (raw_dev.loc[:, idx[:,'mean']] - raw_means)/raw_stds\n",
    "raw_test.loc[:, idx[:,'mean']] = (raw_test.loc[:, idx[:,'mean']] - raw_means)/raw_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If raw or lvl2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_train, raw_dev, raw_test, lvl2_train, lvl2_dev, lvl2_test = [\n",
    "#     simple_imputer(df) for df in (raw_train, raw_dev, raw_test, lvl2_train, lvl2_dev, lvl2_test)\n",
    "# ]\n",
    "# raw_flat_train, raw_flat_dev, raw_flat_test, lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test = [\n",
    "#     df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "#         raw_train, raw_dev, raw_test, lvl2_train, lvl2_dev, lvl2_test\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# for df in lvl2_train, lvl2_dev, lvl2_test, raw_train, raw_dev, raw_test: assert not df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fazeleh/miniconda3/envs/mimic_test/lib/python3.6/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "lvl2_train, lvl2_dev, lvl2_test = [ simple_imputer(df) for df in ( lvl2_train, lvl2_dev, lvl2_test)]\n",
    "lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test = [df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], \n",
    "                                                    columns=['hours_in']) for df in (lvl2_train, lvl2_dev, lvl2_test) ]\n",
    "\n",
    "for df in lvl2_train, lvl2_dev, lvl2_test: assert not df.isnull().any().any()\n",
    "for df in lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test: assert not df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group DataFrames in a list\n",
    "# dataframes = [raw_flat_train, raw_flat_dev, raw_flat_test, lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test ]\n",
    "# raw_flat_train.to_csv('raw_flat_train.csv')\n",
    "# raw_flat_dev.to_csv('raw_flat_dev.csv')\n",
    "# raw_flat_test.to_csv('raw_flat_test.csv')\n",
    "# lvl2_flat_train.to_csv('lvl2_flat_train.csv')\n",
    "# lvl2_flat_dev.to_csv('lvl2_flat_dev.csv')\n",
    "# lvl2_flat_test.to_csv('lvl2_flat_test.csv')\n",
    "# Ys_train.to_csv('Ys_train.csv')\n",
    "# Ys_dev.to_csv('Ys_dev.csv')\n",
    "# Ys_test.to_csv('Ys_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_flat_train = pd.read_csv('load_data/raw_flat_train.csv', index_col=[0,1,2])\n",
    "# raw_flat_dev = pd.read_csv('load_data/raw_flat_dev.csv', index_col=[0,1,2])\n",
    "# raw_flat_test = pd.read_csv('load_data/raw_flat_test.csv', index_col=[0,1,2])\n",
    "# lvl2_flat_train = pd.read_csv('load_data/lvl2_flat_train.csv', index_col=[0,1,2])\n",
    "# lvl2_flat_dev = pd.read_csv('load_data/lvl2_flat_dev.csv', index_col=[0,1,2])\n",
    "# lvl2_flat_test = pd.read_csv('load_data/lvl2_flat_test.csv', index_col=[0,1,2])\n",
    "# Ys_train = pd.read_csv('load_data/Ys_train.csv', index_col=[0])\n",
    "# Ys_dev = pd.read_csv('load_data/Ys_dev.csv', index_col=[0])\n",
    "# Ys_test = pd.read_csv('load_data/Ys_test.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys_test['los_3'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparams = dict({\n",
    "    'C': 0.18544999360231632,\n",
    "    'penalty': 'l2',\n",
    "    'solver': 'liblinear',\n",
    "    'max_iter': 100\n",
    "})\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_only_final(model, hyperparams, X_flat_train, X_flat_dev, X_flat_test):\n",
    "    best_M = model(**hyperparams)\n",
    "    best_M.fit(pd.concat((X_flat_train, X_flat_dev)), pd.concat((Ys_train, Ys_dev))['los_3'])\n",
    "    y_true  = Ys_test['los_3']\n",
    "    y_score = best_M.predict_proba(X_flat_test)[:, 1]\n",
    "    y_pred  = best_M.predict(X_flat_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return best_M, hyperparams, auc, auprc, acc, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_only_final(LogisticRegression,\n",
    "                                hyperparams,\n",
    "                                lvl2_flat_train,\n",
    "                                lvl2_flat_dev,\n",
    "                                lvl2_flat_test) \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have  pytorch LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"Initialize the logistic regression model with a single linear layer.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            input_dim (int): The size of the input feature vector.\n",
    "        \"\"\"\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)  # Binary classification (1 output)\n",
    "        # Metadata initialization\n",
    "        self.init_params = {\"input_dim\": input_dim}\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return torch.sigmoid(self.linear(x))  # Sigmoid to produce probabilities for binary classification\n",
    "\n",
    "\n",
    "# Function to save the model and metadata\n",
    "# def save_model_and_metadata(  # noqa: PLR0913\n",
    "#     model: torch.nn.Module,\n",
    "#     data_split: dict,\n",
    "#     configs: dict,\n",
    "#     train_acc: float,\n",
    "#     test_acc: float,\n",
    "#     train_loss: float,\n",
    "#     test_loss: float,\n",
    "#     optimizer: optim.Optimizer,\n",
    "#     loss: nn.Module,\n",
    "#     n: str\n",
    "# ) -> None:\n",
    "#     \"\"\"Save the model and metadata.\"\"\"\n",
    "#     log_dir = configs[\"run\"][\"log_dir\"]\n",
    "#     Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     with open(f\"{log_dir}/target_model\" + n + \".pkl\", \"wb\") as f:\n",
    "#         torch.save(model.state_dict(), f)\n",
    "\n",
    "#     meta_data = {}\n",
    "\n",
    "#     meta_data[\"init_params\"] = model.init_params if hasattr(model, \"init_params\") else {}\n",
    "#     # meta_data[\"train_indices\"] = data_split[\"train_indices\"]\n",
    "#     # meta_data[\"test_indices\"] = data_split[\"test_indices\"]\n",
    "#     # meta_data[\"num_train\"] = len(data_split[\"train_indices\"])\n",
    "\n",
    "#     # read out optimizer parameters\n",
    "#     meta_data[\"optimizer\"] = {}\n",
    "#     meta_data[\"optimizer\"][\"name\"] = optimizer.__class__.__name__.lower()\n",
    "#     meta_data[\"optimizer\"][\"lr\"] = optimizer.param_groups[0].get(\"lr\", 0)\n",
    "#     meta_data[\"optimizer\"][\"weight_decay\"] = optimizer.param_groups[0].get(\"weight_decay\", 0)\n",
    "#     meta_data[\"optimizer\"][\"momentum\"] = optimizer.param_groups[0].get(\"momentum\", 0)\n",
    "#     meta_data[\"optimizer\"][\"dampening\"] = optimizer.param_groups[0].get(\"dampening\", 0)\n",
    "#     meta_data[\"optimizer\"][\"nesterov\"] = optimizer.param_groups[0].get(\"nesterov\", False)\n",
    "\n",
    "#     # read out loss parameters\n",
    "#     meta_data[\"loss\"] = {}\n",
    "#     meta_data[\"loss\"][\"name\"] = loss.__class__.__name__.lower()\n",
    "\n",
    "#     meta_data[\"batch_size\"] = configs[\"train\"][\"batch_size\"]\n",
    "#     meta_data[\"epochs\"] = configs[\"train\"][\"epochs\"]\n",
    "#     meta_data[\"learning_rate\"] = configs[\"train\"][\"learning_rate\"]\n",
    "#     meta_data[\"weight_decay\"] = configs[\"train\"][\"weight_decay\"]\n",
    "#     meta_data[\"train_acc\"] = train_acc\n",
    "#     meta_data[\"test_acc\"] = test_acc\n",
    "#     meta_data[\"train_loss\"] = train_loss\n",
    "#     meta_data[\"test_loss\"] = test_loss\n",
    "#     meta_data[\"dataset\"] = configs[\"data\"][\"dataset\"]\n",
    "\n",
    "#     with open(f\"{log_dir}/model_metadata\"+ n + \".pkl\", \"wb\") as f:\n",
    "#         pickle.dump(meta_data, f)\n",
    "\n",
    "import torch.nn.init as init\n",
    "# Training and evaluation setup\n",
    "def train_and_save_logistic_regression(X_train, y_train, X_test, y_test, configs):\n",
    "    # Convert the inverse regularization parameter C to weight_decay (regularization strength)\n",
    "\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = X_train.shape[1]  # Assuming X_train is a NumPy array or similar\n",
    "    print(input_dim)\n",
    "    model = LogisticRegressionModel(input_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr =configs[\"train\"][\"learning_rate\"], \n",
    "                            weight_decay=configs[\"train\"][\"weight_decay\"])\n",
    "\n",
    "    # Training loop (max_iter = number of epochs)\n",
    "    epochs =  configs[\"train\"][\"epochs\"]\n",
    "    batch_size = configs[\"train\"][\"batch_size\"]\n",
    "    \n",
    "    inputs = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    # print(f\"inputs {y_train[:,None].shape}\")\n",
    "    labels = torch.tensor(y_train[:,None], dtype=torch.float32)\n",
    "   \n",
    "    # Create a TensorDataset and DataLoader for batch processing\n",
    "    dataset = TensorDataset(inputs, labels)\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch_inputs, batch_labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "            loss = criterion(outputs, batch_labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "\n",
    "            optimizer.step()  # Optimize\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "        \n",
    "        epoch_loss /= len(dataset)\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation on test set\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_test, dtype=torch.float32)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct = (predicted == labels).float().sum()\n",
    "        test_acc = correct / len(labels)\n",
    "        test_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct = (predicted == labels).float().sum()\n",
    "        train_acc = correct / len(labels)\n",
    "        train_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    print(f'Test Accuracy: {test_acc.item():.4f}, Test Loss: {test_loss:.4f}')\n",
    "    print(f'Train Accuracy: {train_acc.item():.4f}, Train Loss: {train_loss:.4f}')\n",
    "    # Save the model and metadata\n",
    "    # save_model_and_metadata(\n",
    "    #     model=model,\n",
    "    #     data_split=data_split,\n",
    "    #     configs=configs,\n",
    "    #     train_acc=train_acc.item(),\n",
    "    #     test_acc=test_acc.item(),\n",
    "    #     train_loss=train_loss,\n",
    "    #     test_loss=test_loss,\n",
    "    #     optimizer=optimizer,\n",
    "    #     loss=criterion,\n",
    "    #     n=n\n",
    "    # )\n",
    "\n",
    "\n",
    "# Example configurations and data split\n",
    "configs = {\n",
    "    \"run\": {\"log_dir\": \"./logs\"},\n",
    "    \"train\": {\"batch_size\": 128, \"epochs\": 10, \"learning_rate\": 0.001, \"weight_decay\": 0.0001},\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# for n, X_flat_train, X_flat_dev, X_flat_test in (\n",
    "#     ('lvl2', lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test),\n",
    "#     ('raw', raw_flat_train, raw_flat_dev, raw_flat_test)):\n",
    "    # results = run_only_final(LogisticRegression,\n",
    "    #                             best_hyperparams,\n",
    "    #                             X_flat_train,\n",
    "    #                             X_flat_dev,\n",
    "    #                             X_flat_test,\n",
    "    #                             'los_3')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "train_and_save_logistic_regression(lvl2_flat_train,\n",
    "                                        Ys_train['los_3'],\n",
    "                                        lvl2_flat_test,\n",
    "                                        Ys_test['los_3'],\n",
    "                                        configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_count = ( Ys_train['los_3'] == 0).sum()  # Count of class 0 samples\n",
    "class_1_count = ( Ys_train['los_3'] == 1).sum()  # Count of class 1 samples\n",
    "pos_weight = class_0_count / class_1_count  # Compute class weight for class 1\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in lvl2_flat_train, lvl2_flat_test : assert not df.isnull().any().any()\n",
    "lvl2_flat_train.values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepBinaryClassifier, self).__init__()\n",
    "        \n",
    "        # Hidden Layer 1\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Batch Normalization\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout for regularization\n",
    "        \n",
    "        # Hidden Layer 2\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Hidden Layer 3\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Output Layer (Binary Classification)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        return self.output(x)  # Do not apply sigmoid here, we will use BCEWithLogitsLoss for stability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(lvl2_flat_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor( Ys_train['los_3'], dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(lvl2_flat_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(Ys_test['los_3'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = DeepBinaryClassifier(input_dim =X_train_tensor.shape[1])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid + BCE loss in a stable manner\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(X_batch)  # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    predicted = torch.sigmoid(test_outputs).round()  # Convert logits to probabilities and round\n",
    "    accuracy = (predicted == y_test_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.7155\n",
      "Epoch [3/50], Loss: 0.6688\n",
      "Epoch [5/50], Loss: 0.6524\n",
      "Epoch [7/50], Loss: 0.6430\n",
      "Epoch [9/50], Loss: 0.6369\n",
      "Epoch [11/50], Loss: 0.6330\n",
      "Epoch [13/50], Loss: 0.6301\n",
      "Epoch [15/50], Loss: 0.6282\n",
      "Epoch [17/50], Loss: 0.6266\n",
      "Epoch [19/50], Loss: 0.6255\n",
      "Epoch [21/50], Loss: 0.6246\n",
      "Epoch [23/50], Loss: 0.6240\n",
      "Epoch [25/50], Loss: 0.6234\n",
      "Epoch [27/50], Loss: 0.6231\n",
      "Epoch [29/50], Loss: 0.6227\n",
      "Epoch [31/50], Loss: 0.6225\n",
      "Epoch [33/50], Loss: 0.6223\n",
      "Epoch [35/50], Loss: 0.6221\n",
      "Epoch [37/50], Loss: 0.6220\n",
      "Epoch [39/50], Loss: 0.6219\n",
      "Epoch [41/50], Loss: 0.6218\n",
      "Epoch [43/50], Loss: 0.6217\n",
      "Epoch [45/50], Loss: 0.6216\n",
      "Epoch [47/50], Loss: 0.6216\n",
      "Epoch [49/50], Loss: 0.6214\n",
      "Test Accuracy: 0.6708, Test Loss: 0.6261\n",
      "Train Accuracy: 0.6754, Train Loss: 0.6206\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"Initialize the logistic regression model with a single linear layer.\"\"\"\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)  # Binary classification (1 output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return torch.sigmoid(self.linear(x))  # Sigmoid for binary classification\n",
    "\n",
    "# Training and evaluation setup\n",
    "def train_and_save_logistic_regression(X_train, y_train, X_test, y_test, configs):\n",
    "    # Initialize the model\n",
    "    input_dim = X_train.shape[1]  # Assuming X_train is a NumPy array or similar\n",
    "    model = LogisticRegressionModel(input_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=configs[\"train\"][\"learning_rate\"], \n",
    "                          weight_decay=configs[\"train\"][\"weight_decay\"])\n",
    "\n",
    "    # Training loop (max_iter = number of epochs)\n",
    "    epochs = configs[\"train\"][\"epochs\"]\n",
    "    batch_size = configs[\"train\"][\"batch_size\"]\n",
    "\n",
    "    inputs = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    labels = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Create a TensorDataset and DataLoader for batch processing\n",
    "    dataset = TensorDataset(inputs, labels)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_inputs, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "            loss = criterion(outputs, batch_labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "        \n",
    "        epoch_loss /= len(dataset)\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation on test set\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct = (predicted == labels).float().sum()\n",
    "        test_acc = correct / len(labels)\n",
    "        test_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct = (predicted == labels).float().sum()\n",
    "        train_acc = correct / len(labels)\n",
    "        train_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    print(f'Test Accuracy: {test_acc.item():.4f}, Test Loss: {test_loss:.4f}')\n",
    "    print(f'Train Accuracy: {train_acc.item():.4f}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "# Example configurations and data split\n",
    "configs = {\n",
    "    \"run\": {\"log_dir\": \"./logs\"},\n",
    "    \"train\": {\"batch_size\": 128, \"epochs\": 50, \"learning_rate\": 1e-4, \"weight_decay\": 5.392},\n",
    "\n",
    "} \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both training and test sets\n",
    "continuous_columns = lvl2_flat_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_continuous = scaler.fit_transform(lvl2_flat_train[continuous_columns])\n",
    "X1_continuous = scaler.transform(lvl2_flat_test[continuous_columns])\n",
    "\n",
    "X_scaled = lvl2_flat_train.copy()\n",
    "X1_scaled = lvl2_flat_test.copy()\n",
    "\n",
    "X_scaled[continuous_columns] = X_continuous\n",
    "X1_scaled[continuous_columns] = X1_continuous\n",
    "\n",
    "X1 = scaler.transform(lvl2_flat_test)\n",
    "X = pd.DataFrame(X_scaled, columns=lvl2_flat_train.columns)\n",
    "X1 = pd.DataFrame(X1_scaled, columns=lvl2_flat_test.columns)\n",
    "train_and_save_logistic_regression(X, Ys_train['los_3'], X1, Ys_test['los_3'], configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alanine aminotransferase           object\n",
       "alanine aminotransferase.1         object\n",
       "alanine aminotransferase.2         object\n",
       "alanine aminotransferase.3         object\n",
       "alanine aminotransferase.4         object\n",
       "alanine aminotransferase.5         object\n",
       "alanine aminotransferase.6         object\n",
       "alanine aminotransferase.7         object\n",
       "alanine aminotransferase.8         object\n",
       "alanine aminotransferase.9         object\n",
       "alanine aminotransferase.10        object\n",
       "alanine aminotransferase.11        object\n",
       "alanine aminotransferase.12        object\n",
       "alanine aminotransferase.13        object\n",
       "alanine aminotransferase.14        object\n",
       "alanine aminotransferase.15        object\n",
       "alanine aminotransferase.16        object\n",
       "alanine aminotransferase.17        object\n",
       "alanine aminotransferase.18        object\n",
       "alanine aminotransferase.19        object\n",
       "alanine aminotransferase.20        object\n",
       "alanine aminotransferase.21        object\n",
       "alanine aminotransferase.22        object\n",
       "alanine aminotransferase.23        object\n",
       "alanine aminotransferase.24        object\n",
       "alanine aminotransferase.25        object\n",
       "alanine aminotransferase.26        object\n",
       "alanine aminotransferase.27        object\n",
       "alanine aminotransferase.28        object\n",
       "alanine aminotransferase.29        object\n",
       "                                    ...  \n",
       "white blood cell count urine.42    object\n",
       "white blood cell count urine.43    object\n",
       "white blood cell count urine.44    object\n",
       "white blood cell count urine.45    object\n",
       "white blood cell count urine.46    object\n",
       "white blood cell count urine.47    object\n",
       "white blood cell count urine.48    object\n",
       "white blood cell count urine.49    object\n",
       "white blood cell count urine.50    object\n",
       "white blood cell count urine.51    object\n",
       "white blood cell count urine.52    object\n",
       "white blood cell count urine.53    object\n",
       "white blood cell count urine.54    object\n",
       "white blood cell count urine.55    object\n",
       "white blood cell count urine.56    object\n",
       "white blood cell count urine.57    object\n",
       "white blood cell count urine.58    object\n",
       "white blood cell count urine.59    object\n",
       "white blood cell count urine.60    object\n",
       "white blood cell count urine.61    object\n",
       "white blood cell count urine.62    object\n",
       "white blood cell count urine.63    object\n",
       "white blood cell count urine.64    object\n",
       "white blood cell count urine.65    object\n",
       "white blood cell count urine.66    object\n",
       "white blood cell count urine.67    object\n",
       "white blood cell count urine.68    object\n",
       "white blood cell count urine.69    object\n",
       "white blood cell count urine.70    object\n",
       "white blood cell count urine.71    object\n",
       "Length: 7488, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvl2_flat_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
