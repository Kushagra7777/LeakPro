{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR DP-sgd example\n",
    "\n",
    "This code runs CIFAR10 and CIFAR100 under DP-sgd. To switch between these two datasets, update the `dataset` field in `train_config.yaml` and the `data_path` field in `audit.yaml` accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure opacus is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opacus in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.15 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.26.3)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.15.1)\n",
      "Requirement already satisfied: opt-einsum>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (3.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->opacus) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the population dataset by concatenating the train and test data. To create the population, we make use of the UserDataset provided in the InputHandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch import cat, tensor\n",
    "import pickle\n",
    "from cifar_handler import CifarInputHandler\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open('train_config.yaml', 'r') as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "    \n",
    "root = train_config[\"data\"][\"data_dir\"]\n",
    "path = os.path.join(os.getcwd(), root)\n",
    "# Load the CIFAR train and test datasets\n",
    "if train_config[\"data\"][\"dataset\"] == \"cifar10\":\n",
    "    trainset = CIFAR10(root=root, train=True, download=True)\n",
    "    testset = CIFAR10(root=root, train=False, download=True)\n",
    "elif train_config[\"data\"][\"dataset\"] == \"cifar100\":\n",
    "    trainset = CIFAR100(root=root, train=True, download=True)\n",
    "    testset = CIFAR100(root=root, train=False, download=True)\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset type\")\n",
    "\n",
    "train_data = tensor(trainset.data).permute(0, 3, 1, 2).float() / 255  # (N, C, H, W)\n",
    "test_data = tensor(testset.data).permute(0, 3, 1, 2).float() / 255\n",
    "\n",
    "# Ensure train and test data looks correct\n",
    "assert train_data.shape[0] == 50000, \"Train data should have 50000 samples\"\n",
    "assert test_data.shape[0] == 10000, \"Test data should have 10000 samples\"\n",
    "assert train_data.shape[1] == 3, \"Data should have 3 channels\"\n",
    "assert test_data.shape[1] == 3, \"Data should have 3 channels\"\n",
    "assert train_data.max() <= 1 and train_data.min() >= 0, \"Data should be normalized\"\n",
    "assert test_data.max() <= 1 and test_data.min() >= 0, \"Data should be normalized\"\n",
    "\n",
    "# Concatenate train and test data into the population\n",
    "data = cat([train_data.clone().detach(), test_data.clone().detach()], dim=0)\n",
    "targets = cat([tensor(trainset.targets), tensor(testset.targets)], dim=0)\n",
    "# Create UserDataset object\n",
    "population_dataset = CifarInputHandler.UserDataset(data, targets)\n",
    "\n",
    "assert len(population_dataset) == 60000, \"Population dataset should have 60000 samples\"\n",
    "\n",
    "# Store the population dataset to be used by LeakPro\n",
    "dataset_name = train_config[\"data\"][\"dataset\"]\n",
    "file_path =  \"data/\"+ dataset_name + \".pkl\"\n",
    "if not os.path.exists(file_path):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(population_dataset, file)\n",
    "        print(f\"Save data to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the population dataset stored, we next create the train and test set that will go in to training the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean: tensor([[[0.4964]],\n",
      "\n",
      "        [[0.4857]],\n",
      "\n",
      "        [[0.4499]]]), Train std: tensor([[[0.2453]],\n",
      "\n",
      "        [[0.2419]],\n",
      "\n",
      "        [[0.2617]]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train_fraction = train_config[\"data\"][\"f_train\"]\n",
    "test_fraction = train_config[\"data\"][\"f_test\"]\n",
    "batch_size = train_config[\"train\"][\"batch_size\"]\n",
    "\n",
    "dataset_size = len(population_dataset)\n",
    "train_size = int(train_fraction * dataset_size)\n",
    "test_size = int(test_fraction * dataset_size)\n",
    "\n",
    "selected_index = np.random.choice(np.arange(dataset_size), train_size + test_size, replace=False)\n",
    "train_indices, test_indices = train_test_split(selected_index, test_size=test_size)\n",
    "\n",
    "train_subset = CifarInputHandler.UserDataset(data[train_indices], targets[train_indices])\n",
    "test_subset = CifarInputHandler.UserDataset(data[test_indices], targets[test_indices], **train_subset.return_params())\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_subset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# Evaluate mean and variance of the train data\n",
    "train_mean = train_subset.mean\n",
    "train_std = train_subset.std\n",
    "print (f\"Train mean: {train_mean}, Train std: {train_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Noise Multiplier Configuration for Privacy Analysis\n",
    "\n",
    "In this code block, we configure the parameters necessary for calculating the noise multiplier using the **Ocapi** library, which we used for differential privacy analysis. \n",
    "\n",
    "- **`target_epsilon`**: The desired epsilon value.\n",
    "- **`target_delta`**: The delta value indicating the risk of privacy loss.\n",
    "- **`sample_rate`**: The rate at which data points are used in training.\n",
    "- **`epochs`**: The number of training epochs for the model.\n",
    "- **`epsilon_tolerance`**: A small margin for the epsilon value,\n",
    "- **`accountant`**: Specifies the method of tracking privacy loss, with \"prv\" referring to the Privacy Accountant for DPSGD.\n",
    "- **`eps_error`**: The allowable error in epsilon calculations\n",
    "- **`max_grad_norm`**: A limit on the gradient norm to ensure the gradients do not explode during training.\n",
    "\n",
    "The most common hyperparameters to tune are `target_epsilon`, `sample_rate`, `noise_multiplier`, and `max_grad_norm`. These parameters should be inputed by the user based on thier need for balancing privacy and utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_dir = \"./target_dpsgd\"\n",
    "delta = 1e-5\n",
    "target_epsilon = 3.5\n",
    "sample_rate = 1/len(train_loader) # already incorporates batchsize\n",
    "    \n",
    "noise_multiplier_dict = {\n",
    "    \"target_epsilon\": target_epsilon,\n",
    "    \"target_delta\": delta,\n",
    "    \"sample_rate\": sample_rate,\n",
    "    \"epochs\": 21,\n",
    "    \"epsilon_tolerance\": 0.01,\n",
    "    \"accountant\": \"prv\",\n",
    "    \"eps_error\": 0.01,\n",
    "    \"max_grad_norm\": 1,\n",
    "}\n",
    "\n",
    "# Create metadata for privacy engine\n",
    "with open(f\"{target_model_dir}/dpsgd_dic.pkl\", \"wb\") as f:\n",
    "    pickle.dump(noise_multiplier_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has privacy violations. Fixing...\n",
      "Model fixed and SGD re-instantiated.\n",
      "Training with DP-SGD\n",
      "Pickle file loaded successfully!\n",
      "Data: {'target_epsilon': 3.5, 'target_delta': 1e-05, 'sample_rate': 0.02631578947368421, 'epochs': 21, 'epsilon_tolerance': 0.01, 'accountant': 'prv', 'eps_error': 0.01, 'max_grad_norm': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "Epoch 1/2:   0%|          | 0/38 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Epoch 1/2: 100%|██████████| 38/38 [00:02<00:00, 13.04it/s]\n",
      "Epoch 2/2: 100%|██████████| 38/38 [00:02<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_module.model.conv1.weight\n",
      "_module.model.bn1.weight\n",
      "_module.model.bn1.bias\n",
      "_module.model.layer1.0.conv1.weight\n",
      "_module.model.layer1.0.bn1.weight\n",
      "_module.model.layer1.0.bn1.bias\n",
      "_module.model.layer1.0.conv2.weight\n",
      "_module.model.layer1.0.bn2.weight\n",
      "_module.model.layer1.0.bn2.bias\n",
      "_module.model.layer1.1.conv1.weight\n",
      "_module.model.layer1.1.bn1.weight\n",
      "_module.model.layer1.1.bn1.bias\n",
      "_module.model.layer1.1.conv2.weight\n",
      "_module.model.layer1.1.bn2.weight\n",
      "_module.model.layer1.1.bn2.bias\n",
      "_module.model.layer2.0.conv1.weight\n",
      "_module.model.layer2.0.bn1.weight\n",
      "_module.model.layer2.0.bn1.bias\n",
      "_module.model.layer2.0.conv2.weight\n",
      "_module.model.layer2.0.bn2.weight\n",
      "_module.model.layer2.0.bn2.bias\n",
      "_module.model.layer2.0.downsample.0.weight\n",
      "_module.model.layer2.0.downsample.1.weight\n",
      "_module.model.layer2.0.downsample.1.bias\n",
      "_module.model.layer2.1.conv1.weight\n",
      "_module.model.layer2.1.bn1.weight\n",
      "_module.model.layer2.1.bn1.bias\n",
      "_module.model.layer2.1.conv2.weight\n",
      "_module.model.layer2.1.bn2.weight\n",
      "_module.model.layer2.1.bn2.bias\n",
      "_module.model.layer3.0.conv1.weight\n",
      "_module.model.layer3.0.bn1.weight\n",
      "_module.model.layer3.0.bn1.bias\n",
      "_module.model.layer3.0.conv2.weight\n",
      "_module.model.layer3.0.bn2.weight\n",
      "_module.model.layer3.0.bn2.bias\n",
      "_module.model.layer3.0.downsample.0.weight\n",
      "_module.model.layer3.0.downsample.1.weight\n",
      "_module.model.layer3.0.downsample.1.bias\n",
      "_module.model.layer3.1.conv1.weight\n",
      "_module.model.layer3.1.bn1.weight\n",
      "_module.model.layer3.1.bn1.bias\n",
      "_module.model.layer3.1.conv2.weight\n",
      "_module.model.layer3.1.bn2.weight\n",
      "_module.model.layer3.1.bn2.bias\n",
      "_module.model.layer4.0.conv1.weight\n",
      "_module.model.layer4.0.bn1.weight\n",
      "_module.model.layer4.0.bn1.bias\n",
      "_module.model.layer4.0.conv2.weight\n",
      "_module.model.layer4.0.bn2.weight\n",
      "_module.model.layer4.0.bn2.bias\n",
      "_module.model.layer4.0.downsample.0.weight\n",
      "_module.model.layer4.0.downsample.1.weight\n",
      "_module.model.layer4.0.downsample.1.bias\n",
      "_module.model.layer4.1.conv1.weight\n",
      "_module.model.layer4.1.bn1.weight\n",
      "_module.model.layer4.1.bn1.bias\n",
      "_module.model.layer4.1.conv2.weight\n",
      "_module.model.layer4.1.bn2.weight\n",
      "_module.model.layer4.1.bn2.bias\n",
      "_module.model.fc.weight\n",
      "_module.model.fc.bias\n",
      "model.conv1.weight\n",
      "model.bn1.weight\n",
      "model.bn1.bias\n",
      "model.bn1.running_mean\n",
      "model.bn1.running_var\n",
      "model.bn1.num_batches_tracked\n",
      "model.layer1.0.conv1.weight\n",
      "model.layer1.0.bn1.weight\n",
      "model.layer1.0.bn1.bias\n",
      "model.layer1.0.bn1.running_mean\n",
      "model.layer1.0.bn1.running_var\n",
      "model.layer1.0.bn1.num_batches_tracked\n",
      "model.layer1.0.conv2.weight\n",
      "model.layer1.0.bn2.weight\n",
      "model.layer1.0.bn2.bias\n",
      "model.layer1.0.bn2.running_mean\n",
      "model.layer1.0.bn2.running_var\n",
      "model.layer1.0.bn2.num_batches_tracked\n",
      "model.layer1.1.conv1.weight\n",
      "model.layer1.1.bn1.weight\n",
      "model.layer1.1.bn1.bias\n",
      "model.layer1.1.bn1.running_mean\n",
      "model.layer1.1.bn1.running_var\n",
      "model.layer1.1.bn1.num_batches_tracked\n",
      "model.layer1.1.conv2.weight\n",
      "model.layer1.1.bn2.weight\n",
      "model.layer1.1.bn2.bias\n",
      "model.layer1.1.bn2.running_mean\n",
      "model.layer1.1.bn2.running_var\n",
      "model.layer1.1.bn2.num_batches_tracked\n",
      "model.layer2.0.conv1.weight\n",
      "model.layer2.0.bn1.weight\n",
      "model.layer2.0.bn1.bias\n",
      "model.layer2.0.bn1.running_mean\n",
      "model.layer2.0.bn1.running_var\n",
      "model.layer2.0.bn1.num_batches_tracked\n",
      "model.layer2.0.conv2.weight\n",
      "model.layer2.0.bn2.weight\n",
      "model.layer2.0.bn2.bias\n",
      "model.layer2.0.bn2.running_mean\n",
      "model.layer2.0.bn2.running_var\n",
      "model.layer2.0.bn2.num_batches_tracked\n",
      "model.layer2.0.downsample.0.weight\n",
      "model.layer2.0.downsample.1.weight\n",
      "model.layer2.0.downsample.1.bias\n",
      "model.layer2.0.downsample.1.running_mean\n",
      "model.layer2.0.downsample.1.running_var\n",
      "model.layer2.0.downsample.1.num_batches_tracked\n",
      "model.layer2.1.conv1.weight\n",
      "model.layer2.1.bn1.weight\n",
      "model.layer2.1.bn1.bias\n",
      "model.layer2.1.bn1.running_mean\n",
      "model.layer2.1.bn1.running_var\n",
      "model.layer2.1.bn1.num_batches_tracked\n",
      "model.layer2.1.conv2.weight\n",
      "model.layer2.1.bn2.weight\n",
      "model.layer2.1.bn2.bias\n",
      "model.layer2.1.bn2.running_mean\n",
      "model.layer2.1.bn2.running_var\n",
      "model.layer2.1.bn2.num_batches_tracked\n",
      "model.layer3.0.conv1.weight\n",
      "model.layer3.0.bn1.weight\n",
      "model.layer3.0.bn1.bias\n",
      "model.layer3.0.bn1.running_mean\n",
      "model.layer3.0.bn1.running_var\n",
      "model.layer3.0.bn1.num_batches_tracked\n",
      "model.layer3.0.conv2.weight\n",
      "model.layer3.0.bn2.weight\n",
      "model.layer3.0.bn2.bias\n",
      "model.layer3.0.bn2.running_mean\n",
      "model.layer3.0.bn2.running_var\n",
      "model.layer3.0.bn2.num_batches_tracked\n",
      "model.layer3.0.downsample.0.weight\n",
      "model.layer3.0.downsample.1.weight\n",
      "model.layer3.0.downsample.1.bias\n",
      "model.layer3.0.downsample.1.running_mean\n",
      "model.layer3.0.downsample.1.running_var\n",
      "model.layer3.0.downsample.1.num_batches_tracked\n",
      "model.layer3.1.conv1.weight\n",
      "model.layer3.1.bn1.weight\n",
      "model.layer3.1.bn1.bias\n",
      "model.layer3.1.bn1.running_mean\n",
      "model.layer3.1.bn1.running_var\n",
      "model.layer3.1.bn1.num_batches_tracked\n",
      "model.layer3.1.conv2.weight\n",
      "model.layer3.1.bn2.weight\n",
      "model.layer3.1.bn2.bias\n",
      "model.layer3.1.bn2.running_mean\n",
      "model.layer3.1.bn2.running_var\n",
      "model.layer3.1.bn2.num_batches_tracked\n",
      "model.layer4.0.conv1.weight\n",
      "model.layer4.0.bn1.weight\n",
      "model.layer4.0.bn1.bias\n",
      "model.layer4.0.bn1.running_mean\n",
      "model.layer4.0.bn1.running_var\n",
      "model.layer4.0.bn1.num_batches_tracked\n",
      "model.layer4.0.conv2.weight\n",
      "model.layer4.0.bn2.weight\n",
      "model.layer4.0.bn2.bias\n",
      "model.layer4.0.bn2.running_mean\n",
      "model.layer4.0.bn2.running_var\n",
      "model.layer4.0.bn2.num_batches_tracked\n",
      "model.layer4.0.downsample.0.weight\n",
      "model.layer4.0.downsample.1.weight\n",
      "model.layer4.0.downsample.1.bias\n",
      "model.layer4.0.downsample.1.running_mean\n",
      "model.layer4.0.downsample.1.running_var\n",
      "model.layer4.0.downsample.1.num_batches_tracked\n",
      "model.layer4.1.conv1.weight\n",
      "model.layer4.1.bn1.weight\n",
      "model.layer4.1.bn1.bias\n",
      "model.layer4.1.bn1.running_mean\n",
      "model.layer4.1.bn1.running_var\n",
      "model.layer4.1.bn1.num_batches_tracked\n",
      "model.layer4.1.conv2.weight\n",
      "model.layer4.1.bn2.weight\n",
      "model.layer4.1.bn2.bias\n",
      "model.layer4.1.bn2.running_mean\n",
      "model.layer4.1.bn2.running_var\n",
      "model.layer4.1.bn2.num_batches_tracked\n",
      "model.fc.weight\n",
      "model.fc.bias\n"
     ]
    }
   ],
   "source": [
    "from torch import save, optim, nn\n",
    "from cifar_handler_dpsgd import CifarInputHandlerDPsgd\n",
    "from target_model_class import ResNet18\n",
    "\n",
    "# Train the model\n",
    "if not os.path.exists(\"target\"):\n",
    "    os.makedirs(\"target\")\n",
    "if train_config[\"data\"][\"dataset\"] == \"cifar10\":\n",
    "    num_classes = 10\n",
    "elif train_config[\"data\"][\"dataset\"] == \"cifar100\":\n",
    "    num_classes = 100\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset name\")\n",
    "\n",
    "# Create instance of target model\n",
    "model = ResNet18(num_classes = num_classes)\n",
    "\n",
    "# Read out the relevant parameters for training\n",
    "lr = train_config[\"train\"][\"learning_rate\"]\n",
    "momentum = train_config[\"train\"][\"momentum\"]\n",
    "epochs = train_config[\"train\"][\"epochs\"]\n",
    "    \n",
    "# Create optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.SGD\n",
    "\n",
    "# train target model\n",
    "train_result = CifarInputHandlerDPsgd().train(dataloader=train_loader,\n",
    "                            model=model,\n",
    "                            criterion=criterion,\n",
    "                            optimizer=optimizer,\n",
    "                            epochs=epochs,\n",
    "                            do_dpsgd=True)\n",
    "\n",
    "from opacus.validators import ModuleValidator\n",
    "_ = ModuleValidator.fix(model)\n",
    "\n",
    "# Get the trained DP-sgd model\n",
    "model = train_result.model\n",
    "\n",
    "from opacus.validators import ModuleValidator\n",
    "_ = ModuleValidator.fix(model)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_result = CifarInputHandlerDPsgd().eval(test_loader, model, criterion)\n",
    "\n",
    "# Store the model and metadata\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# with open(train_config[\"run\"][\"log_dir\"]+\"/target_model.pkl\", \"wb\") as f:\n",
    "#     save(model.state_dict(), f)\n",
    "\n",
    "state_dict = model.cpu().state_dict()\n",
    "cleaned_state_dict = {key.replace(\"_module.\", \"\"): value#.replace(\"module.\", \"\"): value\n",
    "                    for key, value in state_dict.items()}\n",
    "\n",
    "for key, _ in state_dict.items():\n",
    "    print(key)\n",
    "\n",
    "m_ = ResNet18()\n",
    "for key, _ in m_.state_dict().items():\n",
    "    print(key)\n",
    "\n",
    "with open(train_config[\"run\"][\"log_dir\"]+\"/target_model.pkl\", \"wb\") as f:\n",
    "    save(cleaned_state_dict, f)\n",
    "\n",
    "# Create metadata to be used by LeakPro\n",
    "from leakpro import LeakPro\n",
    "meta_data = LeakPro.make_mia_metadata(train_result = train_result,\n",
    "                                      optimizer = optimizer,\n",
    "                                      loss_fn = criterion,\n",
    "                                      dataloader = train_loader,\n",
    "                                      test_result = test_result,\n",
    "                                      epochs = epochs,\n",
    "                                      train_indices = train_indices,\n",
    "                                      test_indices = test_indices,\n",
    "                                      dataset_name = dataset_name)\n",
    "\n",
    "with open(\"target_dpsgd/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight model.conv1.weight True\n",
      "model.bn1.weight model.bn1.weight True\n",
      "model.bn1.bias model.bn1.bias True\n",
      "model.layer1.0.conv1.weight model.bn1.running_mean False\n",
      "model.layer1.0.bn1.weight model.bn1.running_var False\n",
      "model.layer1.0.bn1.bias model.bn1.num_batches_tracked False\n",
      "model.layer1.0.conv2.weight model.layer1.0.conv1.weight False\n",
      "model.layer1.0.bn2.weight model.layer1.0.bn1.weight False\n",
      "model.layer1.0.bn2.bias model.layer1.0.bn1.bias False\n",
      "model.layer1.1.conv1.weight model.layer1.0.bn1.running_mean False\n",
      "model.layer1.1.bn1.weight model.layer1.0.bn1.running_var False\n",
      "model.layer1.1.bn1.bias model.layer1.0.bn1.num_batches_tracked False\n",
      "model.layer1.1.conv2.weight model.layer1.0.conv2.weight False\n",
      "model.layer1.1.bn2.weight model.layer1.0.bn2.weight False\n",
      "model.layer1.1.bn2.bias model.layer1.0.bn2.bias False\n",
      "model.layer2.0.conv1.weight model.layer1.0.bn2.running_mean False\n",
      "model.layer2.0.bn1.weight model.layer1.0.bn2.running_var False\n",
      "model.layer2.0.bn1.bias model.layer1.0.bn2.num_batches_tracked False\n",
      "model.layer2.0.conv2.weight model.layer1.1.conv1.weight False\n",
      "model.layer2.0.bn2.weight model.layer1.1.bn1.weight False\n",
      "model.layer2.0.bn2.bias model.layer1.1.bn1.bias False\n",
      "model.layer2.0.downsample.0.weight model.layer1.1.bn1.running_mean False\n",
      "model.layer2.0.downsample.1.weight model.layer1.1.bn1.running_var False\n",
      "model.layer2.0.downsample.1.bias model.layer1.1.bn1.num_batches_tracked False\n",
      "model.layer2.1.conv1.weight model.layer1.1.conv2.weight False\n",
      "model.layer2.1.bn1.weight model.layer1.1.bn2.weight False\n",
      "model.layer2.1.bn1.bias model.layer1.1.bn2.bias False\n",
      "model.layer2.1.conv2.weight model.layer1.1.bn2.running_mean False\n",
      "model.layer2.1.bn2.weight model.layer1.1.bn2.running_var False\n",
      "model.layer2.1.bn2.bias model.layer1.1.bn2.num_batches_tracked False\n",
      "model.layer3.0.conv1.weight model.layer2.0.conv1.weight False\n",
      "model.layer3.0.bn1.weight model.layer2.0.bn1.weight False\n",
      "model.layer3.0.bn1.bias model.layer2.0.bn1.bias False\n",
      "model.layer3.0.conv2.weight model.layer2.0.bn1.running_mean False\n",
      "model.layer3.0.bn2.weight model.layer2.0.bn1.running_var False\n",
      "model.layer3.0.bn2.bias model.layer2.0.bn1.num_batches_tracked False\n",
      "model.layer3.0.downsample.0.weight model.layer2.0.conv2.weight False\n",
      "model.layer3.0.downsample.1.weight model.layer2.0.bn2.weight False\n",
      "model.layer3.0.downsample.1.bias model.layer2.0.bn2.bias False\n",
      "model.layer3.1.conv1.weight model.layer2.0.bn2.running_mean False\n",
      "model.layer3.1.bn1.weight model.layer2.0.bn2.running_var False\n",
      "model.layer3.1.bn1.bias model.layer2.0.bn2.num_batches_tracked False\n",
      "model.layer3.1.conv2.weight model.layer2.0.downsample.0.weight False\n",
      "model.layer3.1.bn2.weight model.layer2.0.downsample.1.weight False\n",
      "model.layer3.1.bn2.bias model.layer2.0.downsample.1.bias False\n",
      "model.layer4.0.conv1.weight model.layer2.0.downsample.1.running_mean False\n",
      "model.layer4.0.bn1.weight model.layer2.0.downsample.1.running_var False\n",
      "model.layer4.0.bn1.bias model.layer2.0.downsample.1.num_batches_tracked False\n",
      "model.layer4.0.conv2.weight model.layer2.1.conv1.weight False\n",
      "model.layer4.0.bn2.weight model.layer2.1.bn1.weight False\n",
      "model.layer4.0.bn2.bias model.layer2.1.bn1.bias False\n",
      "model.layer4.0.downsample.0.weight model.layer2.1.bn1.running_mean False\n",
      "model.layer4.0.downsample.1.weight model.layer2.1.bn1.running_var False\n",
      "model.layer4.0.downsample.1.bias model.layer2.1.bn1.num_batches_tracked False\n",
      "model.layer4.1.conv1.weight model.layer2.1.conv2.weight False\n",
      "model.layer4.1.bn1.weight model.layer2.1.bn2.weight False\n",
      "model.layer4.1.bn1.bias model.layer2.1.bn2.bias False\n",
      "model.layer4.1.conv2.weight model.layer2.1.bn2.running_mean False\n",
      "model.layer4.1.bn2.weight model.layer2.1.bn2.running_var False\n",
      "model.layer4.1.bn2.bias model.layer2.1.bn2.num_batches_tracked False\n",
      "model.fc.weight model.layer3.0.conv1.weight False\n",
      "model.fc.bias model.layer3.0.bn1.weight False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (key1, _), (key2, _) in zip(cleaned_state_dict.items(), m_.state_dict().items()):\n",
    "    print(key1, key2, key1 == key2)\n",
    "    \n",
    "cleaned_state_dict == m_.state_dict().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_ = ResNet18(num_classes = num_classes)\n",
    "model_priv_ = train_result.model\n",
    "\n",
    "# Print model types to understand what we're comparing\n",
    "print(f\"Original model type: {type(model_)}\")\n",
    "print(f\"DP-SGD model type: {type(model_priv_)}\")\n",
    "\n",
    "# Check if model_priv_ is wrapped by opacus\n",
    "print(f\"Is model_priv_ wrapped by opacus GradSampleModule? {isinstance(model_priv_, torch.nn.Module)}\")\n",
    "\n",
    "# Compare parameter shapes and counts\n",
    "orig_params = sum(p.numel() for p in model_.parameters())\n",
    "priv_params = sum(p.numel() for p in model_priv_.parameters())\n",
    "print(f\"\\nOriginal model parameters: {orig_params:,}\")\n",
    "print(f\"DP-SGD model parameters: {priv_params:,}\")\n",
    "print(f\"Parameter count match: {orig_params == priv_params}\")\n",
    "\n",
    "print(model_priv_)\n",
    "print(model_)\n",
    "\n",
    "from opacus.validators import ModuleValidator\n",
    "_ = ModuleValidator.fix(model_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = train_result.metrics.extra[\"accuracy_history\"]\n",
    "train_loss = train_result.metrics.extra[\"loss_history\"]\n",
    "test_acc = test_result.accuracy\n",
    "test_loss = test_result.loss\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(len(train_loss)-1, test_acc, 'ro', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(len(train_loss)-1, test_loss, 'ro', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_handler import CifarInputHandler\n",
    "# from cifar_handler_dpsgd import CifarInputHandlerDPsgd\n",
    "\n",
    "from leakpro import LeakPro\n",
    "\n",
    "# # Read the config file\n",
    "# config_path = \"audit_dpsgd.yaml\"q\n",
    "\n",
    "# # Prepare leakpro object\n",
    "# leakpro = LeakPro(CifarInputHandler, config_path)\n",
    "\n",
    "# Read the DPsgd config file and prepare LeakPro object for DPsgd\n",
    "config_path = \"audit_dpsgd.yaml\"\n",
    "leakpro = LeakPro(CifarInputHandlerDPsgd, config_path)\n",
    "\n",
    "# Run the audit \n",
    "mia_results_optuna = leakpro.run_audit(return_results=True, use_optuna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize ReportHandler\n",
    "from leakpro.reporting.report_handler import ReportHandler\n",
    "\n",
    "# report_handler = ReportHandler()\n",
    "report_handler = ReportHandler(report_dir=\"./leakpro_output/results\")\n",
    "\n",
    "# Save MIA resuls using report handler\n",
    "for res in mia_results_optuna:\n",
    "    report_handler.save_results(attack_name=res.attack_name, result_data=res, config=res.configs)\n",
    "\n",
    "# # Create the report by compiling the latex text\n",
    "report_handler.create_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
